{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii8j1l81HKNx",
        "outputId": "fce95aa3-69c6-4546-8f79-494fd27e92b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=b2df7825c28b93508f2452869f75750067f34cf0228175ac3f9e4b10d6cd742f\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max, min, avg, count, countDistinct, lit, sum, when, year, month, dayofmonth\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import DateType\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Weather Data Analysis\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "Xyc9OzwCHRTo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = 'data.zip'\n",
        "\n",
        "# Directory where the zip file will be extracted\n",
        "extract_to_dir = 'data'\n",
        "\n",
        "# Check if the extraction directory exists, create if it doesn't\n",
        "if not os.path.exists(extract_to_dir):\n",
        "    os.makedirs(extract_to_dir)\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents into the directory\n",
        "    zip_ref.extractall(extract_to_dir)\n",
        "\n",
        "print(f\"Extracted all files in {zip_file_path} to {extract_to_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFWDYWSrHw5p",
        "outputId": "35adc4b3-6fc8-45a8-b446-cee3fc889b2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted all files in data.zip to data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the data.zip has been extracted into a folder named 'data'\n",
        "weather_df = spark.read.csv(\"data/data/*/*.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame schema to understand your data\n",
        "weather_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SoenoLGHY37",
        "outputId": "1ee2d3d2-78b3-4ba6-ac4f-7e6ea65eb1b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- STATION: long (nullable = true)\n",
            " |-- DATE: date (nullable = true)\n",
            " |-- LATITUDE: double (nullable = true)\n",
            " |-- LONGITUDE: double (nullable = true)\n",
            " |-- ELEVATION: double (nullable = true)\n",
            " |-- NAME: string (nullable = true)\n",
            " |-- TEMP: double (nullable = true)\n",
            " |-- TEMP_ATTRIBUTES: double (nullable = true)\n",
            " |-- DEWP: double (nullable = true)\n",
            " |-- DEWP_ATTRIBUTES: double (nullable = true)\n",
            " |-- SLP: double (nullable = true)\n",
            " |-- SLP_ATTRIBUTES: double (nullable = true)\n",
            " |-- STP: double (nullable = true)\n",
            " |-- STP_ATTRIBUTES: double (nullable = true)\n",
            " |-- VISIB: double (nullable = true)\n",
            " |-- VISIB_ATTRIBUTES: double (nullable = true)\n",
            " |-- WDSP: double (nullable = true)\n",
            " |-- WDSP_ATTRIBUTES: double (nullable = true)\n",
            " |-- MXSPD: double (nullable = true)\n",
            " |-- GUST: double (nullable = true)\n",
            " |-- MAX: double (nullable = true)\n",
            " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
            " |-- MIN: double (nullable = true)\n",
            " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
            " |-- PRCP: double (nullable = true)\n",
            " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
            " |-- SNDP: double (nullable = true)\n",
            " |-- FRSHTT: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hottest_days = weather_df.withColumn(\"YEAR\", year(\"DATE\")) \\\n",
        "    .groupBy(\"YEAR\", \"STATION\", \"NAME\") \\\n",
        "    .agg(max(\"MAX\").alias(\"MAX_TEMP\")) \\\n",
        "    .orderBy(\"YEAR\", \"MAX_TEMP\", ascending=False)\n",
        "\n",
        "hottest_days.show(13)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rETsRfQTH-yV",
        "outputId": "989d5553-1a91-486f-c88a-e81fad374174"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------------+--------+\n",
            "|YEAR|   STATION|            NAME|MAX_TEMP|\n",
            "+----+----------+----------------+--------+\n",
            "|2022|2095099999|      PAJALA, SW|    85.5|\n",
            "|2022|1241099999|      ORLAND, NO|    82.4|\n",
            "|2021|1065099999|    KARASJOK, NO|    88.3|\n",
            "|2021|1062099999|HOPEN ISLAND, NO|    47.3|\n",
            "|2020|1023099999|   BARDUFOSS, NO|    79.9|\n",
            "|2020|1008099999|    LONGYEAR, SV|    71.1|\n",
            "|2019|1023099999|   BARDUFOSS, NO|    78.8|\n",
            "|2019|1008099999|    LONGYEAR, SV|    61.0|\n",
            "|2018|1025099999|      TROMSO, NO|    84.2|\n",
            "|2018|1008099999|    LONGYEAR, SV|    59.2|\n",
            "|2017|1023099999|   BARDUFOSS, NO|    78.6|\n",
            "|2017|1008099999|    LONGYEAR, SV|    55.4|\n",
            "|2016|1023199999|     DRAUGEN, NO|  9999.9|\n",
            "+----+----------+----------------+--------+\n",
            "only showing top 13 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coldest_january = weather_df.withColumn(\"MONTH\", month(\"DATE\")) \\\n",
        "    .filter(col(\"MONTH\") == 1) \\\n",
        "    .select(\"STATION\", \"NAME\", \"DATE\", \"MIN\") \\\n",
        "    .orderBy(\"MIN\").limit(1)\n",
        "\n",
        "coldest_january.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMI5jTwYH_K6",
        "outputId": "6f1a40a7-0925-4837-cc50-7f971a47426c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+----------+-----+\n",
            "|   STATION|         NAME|      DATE|  MIN|\n",
            "+----------+-------------+----------+-----+\n",
            "|1023099999|BARDUFOSS, NO|2017-01-05|-28.3|\n",
            "+----------+-------------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precipitation_2015 = weather_df.filter(year(\"DATE\") == 2015) \\\n",
        "    .select(\"STATION\", \"NAME\", \"DATE\", \"PRCP\") \\\n",
        "    .agg(max(\"PRCP\").alias(\"MAX_PRCP\"), min(\"PRCP\").alias(\"MIN_PRCP\"))\n",
        "\n",
        "precipitation_2015.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw8vhqY0IB8e",
        "outputId": "6bbb2a51-f28b-4235-8b8c-b43ae93d47e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+\n",
            "|MAX_PRCP|MIN_PRCP|\n",
            "+--------+--------+\n",
            "|   99.99|     0.0|\n",
            "+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gust_missing_2019 = weather_df.filter(year(\"DATE\") == 2019) \\\n",
        "    .select((count(when(col(\"GUST\").isNull(), True)) / count(\"*\")).alias(\"MISSING_GUST_PERCENTAGE\"))\n",
        "\n",
        "gust_missing_2019.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF-ipMMlIEQw",
        "outputId": "4043765e-78c5-4c6b-eecd-b2ecb022c046"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+\n",
            "|MISSING_GUST_PERCENTAGE|\n",
            "+-----------------------+\n",
            "|                    0.0|\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year, month, max, min, avg, stddev\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Weather Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Assuming the CSV files have been loaded into `weather_df`\n",
        "temp_stats_2020 = weather_df.filter(year(\"DATE\") == 2020) \\\n",
        "    .withColumn(\"MONTH\", month(\"DATE\")) \\\n",
        "    .groupBy(\"MONTH\") \\\n",
        "    .agg(\n",
        "        avg(\"TEMP\").alias(\"MEAN_TEMP\"),\n",
        "        F.expr(\"percentile_approx(TEMP, 0.5)\").alias(\"MEDIAN_TEMP\"),\n",
        "        stddev(\"TEMP\").alias(\"STDDEV_TEMP\")\n",
        "    )\n",
        "\n",
        "temp_stats_2020.show(12)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LrUVgJ7IHed",
        "outputId": "d09c1b6a-86c7-4c18-b50c-0b5a18b736b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+-----------+------------------+\n",
            "|MONTH|         MEAN_TEMP|MEDIAN_TEMP|       STDDEV_TEMP|\n",
            "+-----+------------------+-----------+------------------+\n",
            "|   12| 19.95483870967742|       20.2| 8.854464048157649|\n",
            "|    1|15.896774193548385|       14.9|12.805172721989297|\n",
            "|    6|47.429999999999986|       46.0| 8.877190347997288|\n",
            "|    3|14.653225806451614|       18.6|15.784789500893568|\n",
            "|    5| 36.21935483870968|       36.0| 8.077246704851957|\n",
            "|    9| 41.84500000000001|       42.5| 5.887660897797833|\n",
            "|    4|23.329999999999995|       26.0|13.022097256170087|\n",
            "|    8| 49.28709677419354|       48.7| 6.548594740281951|\n",
            "|    7| 52.88709677419355|       51.4| 6.663787232915164|\n",
            "|   10|31.529032258064525|       30.7| 9.609052888228808|\n",
            "|   11|29.246666666666663|       29.8|  8.14344837353497|\n",
            "|    2|13.358620689655174|       15.3| 13.09180853418292|\n",
            "+-----+------------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "# Assuming hottest_days DataFrame is already defined and has the columns you mentioned\n",
        "# Concatenate all the columns into a single string column\n",
        "hottest_days_string = hottest_days.withColumn(\n",
        "    \"result\",\n",
        "    concat_ws(\", \", *[col(c).cast(\"string\") for c in hottest_days.columns])\n",
        ")\n",
        "\n",
        "# Select only the concatenated string column\n",
        "hottest_days_single_column = hottest_days_string.select(\"result\")\n",
        "\n",
        "# Now you can save it to a text file\n",
        "hottest_days_single_column.coalesce(1).write.text(\"results/hottest_days.txt\")\n"
      ],
      "metadata": {
        "id": "qSDrYqyqIJj5"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}