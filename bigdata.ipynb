{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii8j1l81HKNx",
        "outputId": "1a85c4c1-c0c9-4b11-cdbf-c1f354963945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ff47bfc5fbfeeac60e96c97ab6b8ecaba6a69998389df2f95594e9681cf01bfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "#Author - Ameya Deshmukh\n",
        "#Date - 03/25/2024\n",
        "#Assignment - BigData\n",
        "\n",
        "#install pyspark\n",
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max, min, avg, count, countDistinct, lit, sum, when, year, month, dayofmonth, stddev, expr, first, row_number\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import DateType\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Weather Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print('Spark Session Initialized')"
      ],
      "metadata": {
        "id": "Xyc9OzwCHRTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc35e8f9-c894-4a7c-863e-7df5775566bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = 'data.zip'\n",
        "\n",
        "# Directory where the zip file will be extracted\n",
        "extract_to_dir = 'data'\n",
        "\n",
        "# Check if the extraction directory exists, create if it doesn't\n",
        "if not os.path.exists(extract_to_dir):\n",
        "    os.makedirs(extract_to_dir)\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents into the directory\n",
        "    zip_ref.extractall(extract_to_dir)\n",
        "\n",
        "print(f\"Extracted all files in {zip_file_path} to {extract_to_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFWDYWSrHw5p",
        "outputId": "b6b3ae5e-e924-4b1d-b9c1-3bcbad763c1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted all files in data.zip to data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the data.zip has been extracted into a folder named 'data'\n",
        "weather_df = spark.read.csv(\"data/data/*/*.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame schema to understand your data\n",
        "weather_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SoenoLGHY37",
        "outputId": "29af73fd-14da-49a4-d9c4-92903a074766"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- STATION: long (nullable = true)\n",
            " |-- DATE: date (nullable = true)\n",
            " |-- LATITUDE: double (nullable = true)\n",
            " |-- LONGITUDE: double (nullable = true)\n",
            " |-- ELEVATION: double (nullable = true)\n",
            " |-- NAME: string (nullable = true)\n",
            " |-- TEMP: double (nullable = true)\n",
            " |-- TEMP_ATTRIBUTES: double (nullable = true)\n",
            " |-- DEWP: double (nullable = true)\n",
            " |-- DEWP_ATTRIBUTES: double (nullable = true)\n",
            " |-- SLP: double (nullable = true)\n",
            " |-- SLP_ATTRIBUTES: double (nullable = true)\n",
            " |-- STP: double (nullable = true)\n",
            " |-- STP_ATTRIBUTES: double (nullable = true)\n",
            " |-- VISIB: double (nullable = true)\n",
            " |-- VISIB_ATTRIBUTES: double (nullable = true)\n",
            " |-- WDSP: double (nullable = true)\n",
            " |-- WDSP_ATTRIBUTES: double (nullable = true)\n",
            " |-- MXSPD: double (nullable = true)\n",
            " |-- GUST: double (nullable = true)\n",
            " |-- MAX: double (nullable = true)\n",
            " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
            " |-- MIN: double (nullable = true)\n",
            " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
            " |-- PRCP: double (nullable = true)\n",
            " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
            " |-- SNDP: double (nullable = true)\n",
            " |-- FRSHTT: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out missing values and add a column for the year\n",
        "filtered_weather_df = weather_df.filter(\n",
        "    (weather_df[\"MAX\"] != 9999.9) &\n",
        "    (weather_df[\"MIN\"] != 9999.9) &\n",
        "    (weather_df[\"PRCP\"] != 99.99) &\n",
        "    (weather_df[\"TEMP\"] != 9999.9)\n",
        ").withColumn(\"YEAR\", year(\"DATE\"))"
      ],
      "metadata": {
        "id": "rETsRfQTH-yV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a window specification partitioned by 'YEAR' with a descending order by the 'MAX' temperature\n",
        "year_partition = Window.partitionBy(\"YEAR\").orderBy((filtered_weather_df[\"MAX\"]).desc())\n",
        "\n",
        "# Assign row numbers to identify the highest temperature record per year\n",
        "highest_temp_by_year = filtered_weather_df.withColumn(\"rank\", row_number().over(year_partition)) \\\n",
        "    .filter(col(\"rank\") == 1) \\\n",
        "    .drop(\"rank\")\n",
        "\n",
        "# Reordering selected columns and displaying the results in descending order by 'DATE'\n",
        "highest_temp_by_year.select(\"STATION\", \"NAME\", \"DATE\", \"MAX\").orderBy(\"DATE\",ascending=False).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMI5jTwYH_K6",
        "outputId": "2ea6ef15-86aa-45c8-9683-e119281a624b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+----------+----+\n",
            "|    STATION|                NAME|      DATE| MAX|\n",
            "+-----------+--------------------+----------+----+\n",
            "| 2095099999|          PAJALA, SW|2022-07-01|85.5|\n",
            "| 1065099999|        KARASJOK, NO|2021-07-05|88.3|\n",
            "| 1023099999|       BARDUFOSS, NO|2020-06-22|79.9|\n",
            "| 1023099999|       BARDUFOSS, NO|2019-07-21|78.8|\n",
            "| 1025099999|          TROMSO, NO|2018-07-29|84.2|\n",
            "| 1023099999|       BARDUFOSS, NO|2017-06-09|78.6|\n",
            "| 1023199999|         DRAUGEN, NO|2016-07-21|77.0|\n",
            "| 1025099999|          TROMSO, NO|2015-07-30|71.6|\n",
            "| 1023099999|       BARDUFOSS, NO|2014-07-10|89.6|\n",
            "| 1001499999|      SORSTOKKEN, NO|2013-08-02|80.6|\n",
            "| 1023099999|       BARDUFOSS, NO|2012-07-05|72.0|\n",
            "| 1046099999|       SORKJOSEN, NO|2011-07-09|87.8|\n",
            "|99407099999|DESTRUCTION IS. W...|2010-08-15|74.8|\n",
            "+-----------+--------------------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the coldest day in January\n",
        "coldest_day_in_jan = filtered_weather_df.withColumn(\"MONTH\", month(\"DATE\")) \\\n",
        "    .filter(col(\"MONTH\") == 1) \\\n",
        "    .select(\"STATION\", \"NAME\", \"DATE\", \"MIN\") \\\n",
        "    .orderBy(\"MIN\").limit(1)\n",
        "\n",
        "coldest_day_in_jan.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw8vhqY0IB8e",
        "outputId": "1f5ff29f-b8f8-4403-9956-b985758620ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+----------+-----+\n",
            "|   STATION|         NAME|      DATE|  MIN|\n",
            "+----------+-------------+----------+-----+\n",
            "|1023099999|BARDUFOSS, NO|2017-01-05|-28.3|\n",
            "+----------+-------------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering data for the year 2015\n",
        "weather_data_2015 = filtered_weather_df.filter(col(\"YEAR\") == 2015)\n",
        "\n",
        "# Compute highest and lowest rainfall amounts in 2015\n",
        "peak_rainfall = weather_data_2015.agg(max(\"PRCP\").alias(\"HIGHEST_RAIN\")).collect()[0][\"HIGHEST_RAIN\"]\n",
        "least_rainfall = weather_data_2015.agg(min(\"PRCP\").alias(\"LOWEST_RAIN\")).collect()[0][\"LOWEST_RAIN\"]\n",
        "\n",
        "# Retrieve station information and date for the highest rainfall\n",
        "peak_rainfall_details = weather_data_2015.filter(col(\"PRCP\") == peak_rainfall).select(\"STATION\", \"NAME\", \"DATE\", \"PRCP\")\n",
        "\n",
        "# Retrieve station information and date for the lowest rainfall\n",
        "least_rainfall_details = weather_data_2015.filter(col(\"PRCP\") == least_rainfall).select(\"STATION\", \"NAME\", \"DATE\", \"PRCP\").limit(1)\n",
        "\n",
        "# Displaying the findings\n",
        "print(\"Highest Rainfall for 2015:\")\n",
        "peak_rainfall_details.show()\n",
        "\n",
        "print(\"Lowest Rainfall for 2015:\")\n",
        "least_rainfall_details.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF-ipMMlIEQw",
        "outputId": "e84dcf53-a7f8-4d0c-e9a5-f00912624560"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest Rainfall for 2015:\n",
            "+----------+----------+----------+----+\n",
            "|   STATION|      NAME|      DATE|PRCP|\n",
            "+----------+----------+----------+----+\n",
            "|1025099999|TROMSO, NO|2015-11-02|2.11|\n",
            "+----------+----------+----------+----+\n",
            "\n",
            "Lowest Rainfall for 2015:\n",
            "+----------+------------+----------+----+\n",
            "|   STATION|        NAME|      DATE|PRCP|\n",
            "+----------+------------+----------+----+\n",
            "|1008099999|LONGYEAR, SV|2015-01-01| 0.0|\n",
            "+----------+------------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fraction of unavailable wind gust readings for 2019\n",
        "gust_absent_2019 = weather_df.filter(year(\"DATE\") == 2019) \\\n",
        "    .select((count(when(col(\"GUST\") == 999.9, True)) / count(\"*\") * 100).alias(\"PERCENTAGE MISSING GUST\"))\n",
        "\n",
        "# Display the percentage of missing gust data in 2019\n",
        "gust_absent_2019.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LrUVgJ7IHed",
        "outputId": "af25ce4b-3c35-4f25-cd4a-21774d51eda3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+\n",
            "|PERCENTAGE MISSING GUST|\n",
            "+-----------------------+\n",
            "|      82.87671232876713|\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select weather data for 2020\n",
        "climate_data_2020 = filtered_weather_df.filter(year(\"DATE\") == 2020)\n",
        "\n",
        "# Add a month column based on the date\n",
        "climate_data_2020 = climate_data_2020.withColumn(\"Month\", month(\"DATE\"))\n",
        "\n",
        "# Compute average, median, mode, and standard deviation of temperature for each month\n",
        "monthly_temperature_stats_2020 = climate_data_2020.groupBy(\"Month\") \\\n",
        "    .agg(avg(\"TEMP\").alias(\"Average Temperature\"),\n",
        "         expr(\"percentile_approx(TEMP, 0.5)\").alias(\"Median Temperature\"),\n",
        "         expr(\"sort_array(collect_list(TEMP))[(cast(size(collect_list(TEMP)) as int) + 1) DIV 2]\").alias(\"Mode Temperature\"),\n",
        "         stddev(\"TEMP\").alias(\"Temperature Stddev\")) \\\n",
        "    .orderBy(\"Month\")\n",
        "\n",
        "# Display the monthly temperature statistics for 2020\n",
        "monthly_temperature_stats_2020.show()\n"
      ],
      "metadata": {
        "id": "qSDrYqyqIJj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dc51d3c-3320-4afb-84fc-769dafd2cceb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------+------------------+----------------+------------------+\n",
            "|Month|Average Temperature|Median Temperature|Mode Temperature|Temperature Stddev|\n",
            "+-----+-------------------+------------------+----------------+------------------+\n",
            "|    1| 15.210169491525424|              14.7|            14.9|12.653031460610185|\n",
            "|    2| 13.577358490566038|              15.3|            15.5|13.186832615404859|\n",
            "|    3| 15.023333333333335|              18.6|            19.9|15.829465837499535|\n",
            "|    4| 23.329999999999995|              26.0|            28.6|13.022097256170087|\n",
            "|    5|  36.21935483870968|              36.0|            36.1| 8.077246704851957|\n",
            "|    6| 47.429999999999986|              46.0|            46.2| 8.877190347997288|\n",
            "|    7|  52.88709677419355|              51.4|            51.7| 6.663787232915164|\n",
            "|    8| 49.376666666666665|              48.7|            49.0| 6.615066692379813|\n",
            "|    9|  40.92727272727273|              39.0|            43.0| 8.161138512375697|\n",
            "|   10| 29.690322580645162|              24.3|            25.2|10.800072679962533|\n",
            "|   11|              31.01|              29.8|            30.4| 7.744883615795801|\n",
            "|   12| 18.642857142857142|              19.5|            20.1| 9.619956934860543|\n",
            "+-----+-------------------+------------------+----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "highest_temp_by_year_df = highest_temp_by_year.select(\"STATION\", \"NAME\", \"DATE\", \"MAX\").orderBy(\"DATE\", ascending=False)\n",
        "\n",
        "# Pandas DataFrame Conversion\n",
        "highest_temp_by_year_pd = highest_temp_by_year.toPandas()\n",
        "coldest_day_in_jan_pd = coldest_day_in_jan.toPandas()\n",
        "peak_rainfall_details_pd = peak_rainfall_details.toPandas()\n",
        "least_rainfall_details_pd = least_rainfall_details.toPandas()\n",
        "gust_absent_2019_pd = gust_absent_2019.toPandas()\n",
        "monthly_temperature_stats_2020_pd = monthly_temperature_stats_2020.toPandas()\n",
        "\n",
        "# Define the file path\n",
        "file_path = \"/result.txt\"\n",
        "\n",
        "# Formatting the output file\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(\"TASK 1:\\n\")\n",
        "    f.write(\"The hottest day for each year.\\n\")\n",
        "    f.write(tabulate(highest_temp_by_year_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "    f.write(\"\\n\\nTASK 2:\\n\")\n",
        "    f.write(\"The coldest day for the month of January across all years.\\n\")\n",
        "    f.write(tabulate(coldest_day_in_jan_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "    f.write(\"\\n\\nTASK 3:\\n\")\n",
        "    f.write(\"Maximum Precipitation for 2015.\\n\")\n",
        "    f.write(tabulate(peak_rainfall_details_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "    f.write(\"\\nMinimum Precipitation for 2015.\\n\")\n",
        "    f.write(tabulate(least_rainfall_details_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "    f.write(\"\\n\\nTASK 4:\\n\")\n",
        "    f.write(\"Percentage of missing values for wind gust for the year 2019.\\n\")\n",
        "    f.write(tabulate(gust_absent_2019_pd, headers='keys', tablefmt='psql', showindex=False))\n",
        "    f.write(\"\\n\\nTASK 5:\\n\")\n",
        "    f.write(\"The mean, median, mode and standard deviation of the temperature for each month for the year 2020.\\n\")\n",
        "    f.write(tabulate(monthly_temperature_stats_2020_pd, headers='keys', tablefmt='psql', showindex=False))"
      ],
      "metadata": {
        "id": "Gqx7LHUrG83P"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}